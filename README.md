# 10 simple rules for reviewing scientific software/methodology

Our objective is to write  - 

Ten simple rules for "reviewing" scientific software/methodology . We wont focus on writing TEN, though. We will write as many as possible. Then redraft to merge what seems similar.

I think we should try to make this a community effort, and point this work out to specific people that have already blogged about the issue (references can be added in ref section) and invite them to contribute (agreed, but we'll be choosy in inviting select people). 

### Relevant questions

- Could a user repeat, replicate, reproduce and/or reuse the method/software?
- Does the method/software offer a better understanding of the problem before offering a solution? 
- Encapsulation: does the method/solution expose the right level of details?
- Is the mehtod/software targeted to the right audience?

### Examples of successful bioinformatics research software

- [Bioconductor](http://bioconductor.org/) and package review guideline. Focus on technical aspects, documentation, re-use of existing appropriate infrastructure. 


### References
- [Managing Expectations When Publishing Tools and Methods for Computational Proteomics](http://pubs.acs.org/doi/abs/10.1021/pr501318d)
- Poisot, Timoth√©e (2015): [Best publishing practices to improve user confidence in scientific software](http://figshare.com/articles/Best_publishing_practices_to_improve_user_confidence_in_scientific_software/1434688) figshare.
- [Ten Simple Rules for the Open Development of Scientific Software[(http://journals.plos.org/ploscollections/article?id=10.1371/journal.pcbi.1002802)
- There are many online references - I will add them here
